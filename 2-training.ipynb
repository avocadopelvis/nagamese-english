{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avocadopelvis/nagamese-english/blob/main/2-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSUyCs23M_H2",
        "outputId": "b67213f7-4b80-44dc-a8a5-ae0611bf72dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install OpenNMT-py 3.x\n",
        "!pip3 install OpenNMT-py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.3-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.1,>=1.13 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.0.1+cu118)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.2 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.12.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.2->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.1,>=1.13->OpenNMT-py) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->OpenNMT-py) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<2.1,>=1.13->OpenNMT-py) (16.0.6)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.1,>=1.13->OpenNMT-py) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.1,>=1.13->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.18.0 fasttext-wheel-0.9.2 portalocker-2.7.0 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.2.0 sacrebleu-2.3.1 waitress-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17eMiHs0rtew",
        "outputId": "4b6bd7a6-a231-435d-b584-38d4d011184b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Your Datasets"
      ],
      "metadata": {
        "id": "vhgIdJn-cLqu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWVOWYedzZ_G",
        "outputId": "401f9481-3154-4ec2-8724-b017d65a477e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Open the folder where you saved your prepapred datasets\n",
        "%cd \"/content/drive/MyDrive/MTP/nmt/\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MTP/nmt\n",
            "compute-bleu.py\t\t\t\t    naga.txt\n",
            "config.yaml\t\t\t\t    naga.txt-filtered.ng\n",
            "eng.txt\t\t\t\t\t    naga.txt-filtered.ng.subword\n",
            "eng.txt-filtered.en\t\t\t    naga.txt-filtered.ng.subword.dev\n",
            "eng.txt-filtered.en.subword\t\t    naga.txt-filtered.ng.subword.test\n",
            "eng.txt-filtered.en.subword.dev\t\t    naga.txt-filtered.ng.subword.train\n",
            "eng.txt-filtered.en.subword.test\t    run\n",
            "eng.txt-filtered.en.subword.test.desubword  source.model\n",
            "eng.txt-filtered.en.subword.train\t    source.vocab\n",
            "en.translated\t\t\t\t    target.model\n",
            "en.translated.desubword\t\t\t    target.vocab\n",
            "models\t\t\t\t\t    train.log\n",
            "MT-Preparation\t\t\t\t    UN.en.translated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPlmhd426B7l"
      },
      "source": [
        "# Create the Training Configuration File\n",
        "\n",
        "The following config file matches most of the recommended values for the Transformer model [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762). As the current dataset is small, we reduced the following values:\n",
        "* `train_steps` - for datasets with a few millions of sentences, consider using a value between 100000 and 200000, or more! Enabling the option `early_stopping` can help stop the training when there is no considerable improvement.\n",
        "* `valid_steps` - 10000 can be good if the value `train_steps` is big enough.\n",
        "* `warmup_steps` - obviously, its value must be less than `train_steps`. Try 4000 and 8000 values.\n",
        "\n",
        "Refer to [OpenNMT-py training parameters](https://opennmt.net/OpenNMT-py/options/train.html) for more details. If you are interested in further explanation of the Transformer model, you can check this article, [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbW7Xek6UDlY"
      },
      "source": [
        "# Create the YAML configuration file\n",
        "# On a regular machine, you can create it manually or with nano\n",
        "# Note here we are using some smaller values because the dataset is small\n",
        "# For larger datasets, consider increasing: train_steps, valid_steps, warmup_steps, save_checkpoint_steps, keep_checkpoint\n",
        "\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: naga.txt-filtered.ng.subword.train\n",
        "        path_tgt: eng.txt-filtered.en.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: naga.txt-filtered.ng.subword.dev\n",
        "        path_tgt: eng.txt-filtered.en.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.fren\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsL4zycvLMUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "112b6f4a-c914-4d36-d460-fb47711215aa"
      },
      "source": [
        "# [Optional] Check the content of the configuration file\n",
        "!cat config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Training files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: naga.txt-filtered.ng.subword.train\n",
            "        path_tgt: eng.txt-filtered.en.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: naga.txt-filtered.ng.subword.dev\n",
            "        path_tgt: eng.txt-filtered.en.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabulary files, generated by onmt_build_vocab\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Vocabulary size - should be the same as in sentence piece\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filter out source/target longer than n if [filtertoolong] enabled\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenization options\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Where to save the log file and the output models/checkpoints\n",
            "log_file: train.log\n",
            "save_model: models/model.fren\n",
            "\n",
            "# Stop training if it does not imporve after n validations\n",
            "early_stopping: 4\n",
            "\n",
            "# Default: 5000 - Save a model checkpoint for each n\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# To save space, limit checkpoints to last n\n",
            "# keep_checkpoint: 3\n",
            "\n",
            "seed: 3435\n",
            "\n",
            "# Default: 100000 - Train the model to max n steps\n",
            "# Increase to 200000 or more for large datasets\n",
            "# For fine-tuning, add up the required steps to the original steps\n",
            "train_steps: 3000\n",
            "\n",
            "# Default: 10000 - Run validation after n steps\n",
            "valid_steps: 1000\n",
            "\n",
            "# Default: 4000 - for large datasets, try up to 8000\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Number of GPUs, and IDs of GPUs\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Optimization\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Model\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0bcqYkEXhRY"
      },
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "For large datasets, it is not feasable to use all words/tokens found in the corpus. Instead, a specific set of vocabulary is extracted from the training dataset, usually betweeen 32k and 100k words. This is the main purpose of the vocabulary building step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuwltKp_VhnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c99acff-2c0b-47ab-b3a3-74b353f0d91a"
      },
      "source": [
        "# Find the number of CPUs/cores on the machine\n",
        "!nproc --all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2GV1PgyUsJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8314637b-78ec-42a0-faf4-73784b7b2443"
      },
      "source": [
        "# Build Vocabulary\n",
        "\n",
        "# -config: path to your config.yaml file\n",
        "# -n_sample: use -1 to build vocabulary on all the segment in the training dataset\n",
        "# -num_threads: change it to match the number of CPUs to run it faster\n",
        "\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-08-22 09:48:28,076 INFO] Counter vocab from -1 samples.\n",
            "[2023-08-22 09:48:28,076 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2023-08-22 09:48:30,146 INFO] Counters src: 2855\n",
            "[2023-08-22 09:48:30,146 INFO] Counters tgt: 5359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncWyNtxiO_Ov"
      },
      "source": [
        "From the **Runtime menu** > **Change runtime type**, make sure that the \"**Hardware accelerator**\" is \"**GPU**\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMMPeS-pSV8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7570c9b-21a3-4665-f160-2e653cc29c99"
      },
      "source": [
        "# Check if the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-9a89ab1c-a6e0-31ac-a1d3-f34c1ceae391)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3rVQhd4NXNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e3e632-3503-429b-89ad-34a880622232"
      },
      "source": [
        "# Check if the GPU is visible to PyTorch\n",
        "\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0]/1024**2, \"out of:\", gpu_memory[1]/1024**2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14998.8125 out of: 15101.8125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aCxETSnXcL-"
      },
      "source": [
        "# Training\n",
        "\n",
        "Now, start training your NMT model! üéâ üéâ üéâ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf drive/MyDrive/MTP/nmt/models/"
      ],
      "metadata": {
        "id": "HZd1o1kIb6Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prJCKA2CP-dl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab75cc5-cda6-43d1-a65f-3919e44cc046"
      },
      "source": [
        "# Train the NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-08-22 09:53:39,521 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2023-08-22 09:53:40,365 INFO] Parsed 2 corpora from -data.\n",
            "[2023-08-22 09:53:40,365 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2023-08-22 09:53:40,397 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', ',', '‚ñÅlaga', '.', '‚ñÅkhan', '‚ñÅke', '‚ñÅaru']\n",
            "[2023-08-22 09:53:40,397 INFO] The decoder start token is: <s>\n",
            "[2023-08-22 09:53:40,397 INFO] Building model...\n",
            "[2023-08-22 09:53:41,073 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2023-08-22 09:53:41,074 INFO] Non quantized layer compute is fp16\n",
            "[2023-08-22 09:53:41,341 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(2864, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5368, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=5368, bias=True)\n",
            ")\n",
            "[2023-08-22 09:53:41,345 INFO] encoder: 20354048\n",
            "[2023-08-22 09:53:41,345 INFO] decoder: 30687480\n",
            "[2023-08-22 09:53:41,345 INFO] * number of parameters: 51041528\n",
            "[2023-08-22 09:53:41,348 INFO] Trainable parameters = {'torch.float32': 51041528, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-08-22 09:53:41,349 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2023-08-22 09:53:41,349 INFO]  * src vocab size = 2864\n",
            "[2023-08-22 09:53:41,349 INFO]  * tgt vocab size = 5368\n",
            "[2023-08-22 09:53:41,355 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2023-08-22 09:53:41,356 INFO] Starting training on GPU: [0]\n",
            "[2023-08-22 09:53:41,356 INFO] Start training loop and validate every 1000 steps...\n",
            "[2023-08-22 09:53:41,356 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2023-08-22 09:53:41,664 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2023-08-22 09:53:42,061 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2023-08-22 09:53:42,359 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2023-08-22 09:53:42,807 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2023-08-22 09:53:43,122 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2023-08-22 09:53:43,631 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2023-08-22 09:53:43,954 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2023-08-22 09:53:44,257 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2023-08-22 09:53:44,848 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2023-08-22 09:53:45,156 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2023-08-22 09:53:45,457 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2023-08-22 09:53:46,145 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2023-08-22 09:53:46,452 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2023-08-22 09:53:46,748 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2023-08-22 09:53:47,061 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2023-08-22 09:53:47,814 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2023-08-22 09:53:48,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2023-08-22 09:53:48,413 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2023-08-22 09:53:48,707 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2023-08-22 09:53:49,009 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2023-08-22 09:53:49,301 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2023-08-22 09:53:50,192 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2023-08-22 09:53:50,722 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2023-08-22 09:53:51,258 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2023-08-22 09:53:51,799 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2023-08-22 09:53:52,354 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2023-08-22 09:53:52,893 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2023-08-22 09:53:54,341 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2023-08-22 09:53:54,645 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2023-08-22 09:53:54,943 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2023-08-22 09:53:55,251 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2023-08-22 09:53:55,548 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2023-08-22 09:53:55,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2023-08-22 09:53:56,151 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2023-08-22 09:53:56,456 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2023-08-22 09:53:57,725 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2023-08-22 09:53:58,026 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2023-08-22 09:55:07,653 INFO] Step 100/ 3000; acc: 9.6; ppl: 872.2; xent: 6.8; lr: 0.00028; sents:   49628; bsz: 3878/3628/124; 17973/16818 tok/s;     86 sec;\n",
            "[2023-08-22 09:56:04,530 INFO] Step 200/ 3000; acc: 27.8; ppl: 122.1; xent: 4.8; lr: 0.00056; sents:   48302; bsz: 3883/3633/121; 27311/25550 tok/s;    143 sec;\n",
            "[2023-08-22 09:57:02,355 INFO] Step 300/ 3000; acc: 45.2; ppl:  37.4; xent: 3.6; lr: 0.00084; sents:   49630; bsz: 3868/3618/124; 26760/25031 tok/s;    201 sec;\n",
            "[2023-08-22 09:57:57,609 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2023-08-22 09:57:57,938 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2023-08-22 09:57:58,242 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2023-08-22 09:57:58,532 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2023-08-22 09:57:58,824 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2023-08-22 09:57:59,121 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2023-08-22 09:57:59,409 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2023-08-22 09:58:02,715 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2023-08-22 09:58:03,086 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2023-08-22 09:58:03,376 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2023-08-22 09:58:03,650 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2023-08-22 09:58:03,920 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2023-08-22 09:58:04,203 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2023-08-22 09:58:04,479 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2023-08-22 09:58:04,755 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2023-08-22 09:58:05,031 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2023-08-22 09:58:05,327 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2023-08-22 09:58:05,606 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2023-08-22 09:58:05,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2023-08-22 09:58:06,168 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2023-08-22 09:58:06,475 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2023-08-22 09:58:10,648 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2023-08-22 09:58:10,955 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2023-08-22 09:58:11,252 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2023-08-22 09:58:11,568 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 63\n",
            "[2023-08-22 09:58:11,864 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 64\n",
            "[2023-08-22 09:58:12,163 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 65\n",
            "[2023-08-22 09:58:12,478 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 66\n",
            "[2023-08-22 09:58:12,776 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 67\n",
            "[2023-08-22 09:58:13,194 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 68\n",
            "[2023-08-22 09:58:13,768 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 69\n",
            "[2023-08-22 09:58:14,336 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 70\n",
            "[2023-08-22 09:58:14,910 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 71\n",
            "[2023-08-22 09:58:15,476 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 72\n",
            "[2023-08-22 09:58:16,034 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 73\n",
            "[2023-08-22 09:58:16,603 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 74\n",
            "[2023-08-22 09:58:17,128 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 75\n",
            "[2023-08-22 09:58:17,441 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 76\n",
            "[2023-08-22 09:58:38,138 INFO] Step 400/ 3000; acc: 69.6; ppl:  11.8; xent: 2.5; lr: 0.00112; sents:   49596; bsz: 3880/3635/124; 16201/15179 tok/s;    297 sec;\n",
            "[2023-08-22 09:59:36,292 INFO] Step 500/ 3000; acc: 88.3; ppl:   5.9; xent: 1.8; lr: 0.00140; sents:   49153; bsz: 3879/3628/123; 26680/24954 tok/s;    355 sec;\n",
            "[2023-08-22 10:00:33,678 INFO] Step 600/ 3000; acc: 94.9; ppl:   4.6; xent: 1.5; lr: 0.00168; sents:   49742; bsz: 3869/3623/124; 26970/25256 tok/s;    412 sec;\n",
            "[2023-08-22 10:01:31,496 INFO] Step 700/ 3000; acc: 96.2; ppl:   4.4; xent: 1.5; lr: 0.00196; sents:   48489; bsz: 3884/3630/121; 26869/25112 tok/s;    470 sec;\n",
            "[2023-08-22 10:02:24,491 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 77\n",
            "[2023-08-22 10:02:25,086 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 78\n",
            "[2023-08-22 10:02:25,624 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 79\n",
            "[2023-08-22 10:02:26,136 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 80\n",
            "[2023-08-22 10:02:26,488 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 81\n",
            "[2023-08-22 10:02:26,779 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 82\n",
            "[2023-08-22 10:02:27,065 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 83\n",
            "[2023-08-22 10:02:27,343 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 84\n",
            "[2023-08-22 10:02:27,623 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 85\n",
            "[2023-08-22 10:02:27,913 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 86\n",
            "[2023-08-22 10:02:28,181 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 87\n",
            "[2023-08-22 10:02:28,448 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 88\n",
            "[2023-08-22 10:02:28,723 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 89\n",
            "[2023-08-22 10:02:31,858 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 90\n",
            "[2023-08-22 10:02:32,133 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 91\n",
            "[2023-08-22 10:02:32,410 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 92\n",
            "[2023-08-22 10:02:32,685 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 93\n",
            "[2023-08-22 10:02:32,982 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 94\n",
            "[2023-08-22 10:02:33,270 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 95\n",
            "[2023-08-22 10:02:33,549 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 96\n",
            "[2023-08-22 10:02:33,831 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 97\n",
            "[2023-08-22 10:02:34,116 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 98\n",
            "[2023-08-22 10:02:34,395 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 99\n",
            "[2023-08-22 10:02:34,666 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 100\n",
            "[2023-08-22 10:02:34,949 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 101\n",
            "[2023-08-22 10:02:35,215 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 102\n",
            "[2023-08-22 10:02:35,483 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 103\n",
            "[2023-08-22 10:02:35,749 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 104\n",
            "[2023-08-22 10:02:36,017 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 105\n",
            "[2023-08-22 10:02:40,475 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 106\n",
            "[2023-08-22 10:02:40,738 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 107\n",
            "[2023-08-22 10:02:41,005 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 108\n",
            "[2023-08-22 10:02:41,282 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 109\n",
            "[2023-08-22 10:02:41,548 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 110\n",
            "[2023-08-22 10:02:41,816 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 111\n",
            "[2023-08-22 10:02:42,078 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 112\n",
            "[2023-08-22 10:02:42,346 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 113\n",
            "[2023-08-22 10:02:42,609 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 114\n",
            "[2023-08-22 10:03:06,986 INFO] Step 800/ 3000; acc: 97.2; ppl:   4.1; xent: 1.4; lr: 0.00224; sents:   49651; bsz: 3878/3633/124; 16245/15220 tok/s;    566 sec;\n",
            "[2023-08-22 10:04:04,956 INFO] Step 900/ 3000; acc: 96.7; ppl:   4.2; xent: 1.4; lr: 0.00252; sents:   48788; bsz: 3883/3625/122; 26791/25015 tok/s;    624 sec;\n",
            "[2023-08-22 10:05:02,591 INFO] Step 1000/ 3000; acc: 96.5; ppl:   4.2; xent: 1.4; lr: 0.00279; sents:   48886; bsz: 3879/3632/122; 26925/25205 tok/s;    681 sec;\n",
            "[2023-08-22 10:05:04,850 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 2.25722599029541 s.\n",
            "[2023-08-22 10:05:04,851 INFO] Train perplexity: 14.4616\n",
            "[2023-08-22 10:05:04,851 INFO] Train accuracy: 72.207\n",
            "[2023-08-22 10:05:04,851 INFO] Sentences processed: 491865\n",
            "[2023-08-22 10:05:04,851 INFO] Average bsz: 3878/3629/123\n",
            "[2023-08-22 10:05:04,851 INFO] Validation perplexity: 104.985\n",
            "[2023-08-22 10:05:04,851 INFO] Validation accuracy: 39.9414\n",
            "[2023-08-22 10:05:04,851 INFO] Model is improving ppl: inf --> 104.985.\n",
            "[2023-08-22 10:05:04,851 INFO] Model is improving acc: -inf --> 39.9414.\n",
            "[2023-08-22 10:05:04,854 INFO] Saving checkpoint models/model.fren_step_1000.pt\n",
            "[2023-08-22 10:06:04,813 INFO] Step 1100/ 3000; acc: 97.1; ppl:   4.0; xent: 1.4; lr: 0.00266; sents:   48951; bsz: 3872/3625/122; 24892/23301 tok/s;    743 sec;\n",
            "[2023-08-22 10:06:55,372 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 115\n",
            "[2023-08-22 10:06:55,894 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 116\n",
            "[2023-08-22 10:06:56,430 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 117\n",
            "[2023-08-22 10:06:56,722 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 118\n",
            "[2023-08-22 10:06:57,000 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 119\n",
            "[2023-08-22 10:06:57,288 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 120\n",
            "[2023-08-22 10:06:57,582 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 121\n",
            "[2023-08-22 10:06:57,874 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 122\n",
            "[2023-08-22 10:06:58,150 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 123\n",
            "[2023-08-22 10:06:58,447 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 124\n",
            "[2023-08-22 10:06:58,727 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 125\n",
            "[2023-08-22 10:06:59,002 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 126\n",
            "[2023-08-22 10:06:59,285 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 127\n",
            "[2023-08-22 10:06:59,577 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 128\n",
            "[2023-08-22 10:06:59,859 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 129\n",
            "[2023-08-22 10:07:00,137 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 130\n",
            "[2023-08-22 10:07:00,437 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 131\n",
            "[2023-08-22 10:07:00,712 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 132\n",
            "[2023-08-22 10:07:01,117 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 133\n",
            "[2023-08-22 10:07:01,635 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 134\n",
            "[2023-08-22 10:07:02,153 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 135\n",
            "[2023-08-22 10:07:02,688 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 136\n",
            "[2023-08-22 10:07:06,782 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 137\n",
            "[2023-08-22 10:07:07,050 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 138\n",
            "[2023-08-22 10:07:07,326 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 139\n",
            "[2023-08-22 10:07:07,598 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 140\n",
            "[2023-08-22 10:07:07,875 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 141\n",
            "[2023-08-22 10:07:08,141 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 142\n",
            "[2023-08-22 10:07:08,402 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 143\n",
            "[2023-08-22 10:07:08,663 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 144\n",
            "[2023-08-22 10:07:08,930 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 145\n",
            "[2023-08-22 10:07:09,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 146\n",
            "[2023-08-22 10:07:09,461 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 147\n",
            "[2023-08-22 10:07:09,730 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 148\n",
            "[2023-08-22 10:07:09,993 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 149\n",
            "[2023-08-22 10:07:10,252 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 150\n",
            "[2023-08-22 10:07:10,506 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 151\n",
            "[2023-08-22 10:07:10,776 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 152\n",
            "[2023-08-22 10:07:38,399 INFO] Step 1200/ 3000; acc: 98.2; ppl:   3.8; xent: 1.3; lr: 0.00255; sents:   49966; bsz: 3877/3633/125; 16570/15528 tok/s;    837 sec;\n",
            "[2023-08-22 10:08:36,126 INFO] Step 1300/ 3000; acc: 98.5; ppl:   3.8; xent: 1.3; lr: 0.00245; sents:   48860; bsz: 3873/3622/122; 26838/25098 tok/s;    895 sec;\n",
            "[2023-08-22 10:09:33,623 INFO] Step 1400/ 3000; acc: 98.6; ppl:   3.7; xent: 1.3; lr: 0.00236; sents:   50122; bsz: 3876/3635/125; 26962/25285 tok/s;    952 sec;\n",
            "[2023-08-22 10:10:31,262 INFO] Step 1500/ 3000; acc: 98.3; ppl:   3.8; xent: 1.3; lr: 0.00228; sents:   48601; bsz: 3877/3627/122; 26903/25168 tok/s;   1010 sec;\n",
            "[2023-08-22 10:11:19,577 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 153\n",
            "[2023-08-22 10:11:19,917 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 154\n",
            "[2023-08-22 10:11:20,217 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 155\n",
            "[2023-08-22 10:11:20,507 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 156\n",
            "[2023-08-22 10:11:20,811 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 157\n",
            "[2023-08-22 10:11:21,095 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 158\n",
            "[2023-08-22 10:11:21,376 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 159\n",
            "[2023-08-22 10:11:21,663 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 160\n",
            "[2023-08-22 10:11:21,960 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 161\n",
            "[2023-08-22 10:11:22,240 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 162\n",
            "[2023-08-22 10:11:22,732 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 163\n",
            "[2023-08-22 10:11:23,246 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 164\n",
            "[2023-08-22 10:11:23,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 165\n",
            "[2023-08-22 10:11:24,276 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 166\n",
            "[2023-08-22 10:11:24,811 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 167\n",
            "[2023-08-22 10:11:25,337 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 168\n",
            "[2023-08-22 10:11:29,156 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 169\n",
            "[2023-08-22 10:11:29,439 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 170\n",
            "[2023-08-22 10:11:29,721 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 171\n",
            "[2023-08-22 10:11:30,016 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 172\n",
            "[2023-08-22 10:11:30,313 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 173\n",
            "[2023-08-22 10:11:30,601 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 174\n",
            "[2023-08-22 10:11:30,889 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 175\n",
            "[2023-08-22 10:11:31,177 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 176\n",
            "[2023-08-22 10:11:31,456 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 177\n",
            "[2023-08-22 10:11:31,732 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 178\n",
            "[2023-08-22 10:11:32,035 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 179\n",
            "[2023-08-22 10:11:32,329 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 180\n",
            "[2023-08-22 10:11:32,613 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 181\n",
            "[2023-08-22 10:11:32,889 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 182\n",
            "[2023-08-22 10:11:33,169 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 183\n",
            "[2023-08-22 10:11:33,450 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 184\n",
            "[2023-08-22 10:11:37,952 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 185\n",
            "[2023-08-22 10:11:38,462 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 186\n",
            "[2023-08-22 10:11:38,979 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 187\n",
            "[2023-08-22 10:11:39,491 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 188\n",
            "[2023-08-22 10:11:39,987 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 189\n",
            "[2023-08-22 10:11:40,303 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 190\n",
            "[2023-08-22 10:12:06,056 INFO] Step 1600/ 3000; acc: 99.4; ppl:   3.5; xent: 1.3; lr: 0.00221; sents:   49281; bsz: 3885/3631/123; 16392/15323 tok/s;   1105 sec;\n",
            "[2023-08-22 10:13:03,897 INFO] Step 1700/ 3000; acc: 98.9; ppl:   3.6; xent: 1.3; lr: 0.00214; sents:   48858; bsz: 3881/3630/122; 26839/25103 tok/s;   1163 sec;\n",
            "[2023-08-22 10:14:01,235 INFO] Step 1800/ 3000; acc: 98.7; ppl:   3.7; xent: 1.3; lr: 0.00208; sents:   49847; bsz: 3874/3629/125; 27029/25319 tok/s;   1220 sec;\n",
            "[2023-08-22 10:14:58,694 INFO] Step 1900/ 3000; acc: 99.1; ppl:   3.6; xent: 1.3; lr: 0.00203; sents:   48247; bsz: 3881/3631/121; 27016/25276 tok/s;   1277 sec;\n",
            "[2023-08-22 10:15:44,529 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 191\n",
            "[2023-08-22 10:15:45,060 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 192\n",
            "[2023-08-22 10:15:45,595 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 193\n",
            "[2023-08-22 10:15:48,944 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 194\n",
            "[2023-08-22 10:15:49,214 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 195\n",
            "[2023-08-22 10:15:49,496 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 196\n",
            "[2023-08-22 10:15:49,767 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 197\n",
            "[2023-08-22 10:15:50,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 198\n",
            "[2023-08-22 10:15:50,330 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 199\n",
            "[2023-08-22 10:15:50,612 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 200\n",
            "[2023-08-22 10:15:50,884 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 201\n",
            "[2023-08-22 10:15:51,155 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 202\n",
            "[2023-08-22 10:15:51,440 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 203\n",
            "[2023-08-22 10:15:51,731 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 204\n",
            "[2023-08-22 10:15:52,000 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 205\n",
            "[2023-08-22 10:15:52,274 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 206\n",
            "[2023-08-22 10:15:55,915 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 207\n",
            "[2023-08-22 10:15:56,183 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 208\n",
            "[2023-08-22 10:15:56,460 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 209\n",
            "[2023-08-22 10:15:56,737 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 210\n",
            "[2023-08-22 10:15:57,016 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 211\n",
            "[2023-08-22 10:15:57,286 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 212\n",
            "[2023-08-22 10:15:57,757 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 213\n",
            "[2023-08-22 10:15:58,261 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 214\n",
            "[2023-08-22 10:15:58,774 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 215\n",
            "[2023-08-22 10:15:59,287 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 216\n",
            "[2023-08-22 10:15:59,803 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 217\n",
            "[2023-08-22 10:16:00,302 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 218\n",
            "[2023-08-22 10:16:00,802 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 219\n",
            "[2023-08-22 10:16:01,297 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 220\n",
            "[2023-08-22 10:16:01,640 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 221\n",
            "[2023-08-22 10:16:01,912 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 222\n",
            "[2023-08-22 10:16:06,079 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 223\n",
            "[2023-08-22 10:16:06,345 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 224\n",
            "[2023-08-22 10:16:06,610 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 225\n",
            "[2023-08-22 10:16:06,884 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 226\n",
            "[2023-08-22 10:16:07,152 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 227\n",
            "[2023-08-22 10:16:07,422 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 228\n",
            "[2023-08-22 10:16:33,899 INFO] Step 2000/ 3000; acc: 99.0; ppl:   3.6; xent: 1.3; lr: 0.00198; sents:   49986; bsz: 3872/3622/125; 16269/15220 tok/s;   1373 sec;\n",
            "[2023-08-22 10:16:34,324 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 0.42319178581237793 s.\n",
            "[2023-08-22 10:16:34,325 INFO] Train perplexity: 7.32413\n",
            "[2023-08-22 10:16:34,325 INFO] Train accuracy: 85.3925\n",
            "[2023-08-22 10:16:34,325 INFO] Sentences processed: 984584\n",
            "[2023-08-22 10:16:34,325 INFO] Average bsz: 3877/3628/123\n",
            "[2023-08-22 10:16:34,325 INFO] Validation perplexity: 103.144\n",
            "[2023-08-22 10:16:34,325 INFO] Validation accuracy: 41.7962\n",
            "[2023-08-22 10:16:34,325 INFO] Model is improving ppl: 104.985 --> 103.144.\n",
            "[2023-08-22 10:16:34,325 INFO] Model is improving acc: 39.9414 --> 41.7962.\n",
            "[2023-08-22 10:16:34,328 INFO] Saving checkpoint models/model.fren_step_2000.pt\n",
            "[2023-08-22 10:17:34,723 INFO] Step 2100/ 3000; acc: 99.3; ppl:   3.5; xent: 1.3; lr: 0.00193; sents:   49941; bsz: 3878/3633/125; 25502/23895 tok/s;   1433 sec;\n",
            "[2023-08-22 10:18:32,155 INFO] Step 2200/ 3000; acc: 98.9; ppl:   3.6; xent: 1.3; lr: 0.00188; sents:   48323; bsz: 3879/3619/121; 27014/25209 tok/s;   1491 sec;\n",
            "[2023-08-22 10:19:29,554 INFO] Step 2300/ 3000; acc: 99.0; ppl:   3.6; xent: 1.3; lr: 0.00184; sents:   49516; bsz: 3873/3630/124; 26989/25300 tok/s;   1548 sec;\n",
            "[2023-08-22 10:20:13,199 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 229\n",
            "[2023-08-22 10:20:13,496 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 230\n",
            "[2023-08-22 10:20:13,798 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 231\n",
            "[2023-08-22 10:20:14,089 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 232\n",
            "[2023-08-22 10:20:17,134 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 233\n",
            "[2023-08-22 10:20:17,416 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 234\n",
            "[2023-08-22 10:20:17,696 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 235\n",
            "[2023-08-22 10:20:17,977 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 236\n",
            "[2023-08-22 10:20:18,294 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 237\n",
            "[2023-08-22 10:20:18,822 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 238\n",
            "[2023-08-22 10:20:19,333 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 239\n",
            "[2023-08-22 10:20:19,867 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 240\n",
            "[2023-08-22 10:20:20,398 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 241\n",
            "[2023-08-22 10:20:20,917 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 242\n",
            "[2023-08-22 10:20:21,437 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 243\n",
            "[2023-08-22 10:20:21,951 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 244\n",
            "[2023-08-22 10:20:22,319 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 245\n",
            "[2023-08-22 10:20:26,047 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 246\n",
            "[2023-08-22 10:20:26,324 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 247\n",
            "[2023-08-22 10:20:26,602 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 248\n",
            "[2023-08-22 10:20:26,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 249\n",
            "[2023-08-22 10:20:27,174 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 250\n",
            "[2023-08-22 10:20:27,455 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 251\n",
            "[2023-08-22 10:20:27,730 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 252\n",
            "[2023-08-22 10:20:28,008 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 253\n",
            "[2023-08-22 10:20:28,297 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 254\n",
            "[2023-08-22 10:20:28,573 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 255\n",
            "[2023-08-22 10:20:28,854 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 256\n",
            "[2023-08-22 10:20:29,147 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 257\n",
            "[2023-08-22 10:20:29,421 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 258\n",
            "[2023-08-22 10:20:29,694 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 259\n",
            "[2023-08-22 10:20:29,971 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 260\n",
            "[2023-08-22 10:20:30,254 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 261\n",
            "[2023-08-22 10:20:30,532 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 262\n",
            "[2023-08-22 10:20:35,391 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 263\n",
            "[2023-08-22 10:20:35,661 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 264\n",
            "[2023-08-22 10:20:35,937 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 265\n",
            "[2023-08-22 10:20:36,210 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 266\n",
            "[2023-08-22 10:21:06,102 INFO] Step 2400/ 3000; acc: 99.3; ppl:   3.5; xent: 1.3; lr: 0.00180; sents:   49573; bsz: 3880/3633/124; 16073/15054 tok/s;   1645 sec;\n",
            "[2023-08-22 10:22:03,872 INFO] Step 2500/ 3000; acc: 99.1; ppl:   3.5; xent: 1.3; lr: 0.00177; sents:   49578; bsz: 3878/3630/124; 26853/25137 tok/s;   1703 sec;\n",
            "[2023-08-22 10:23:01,064 INFO] Step 2600/ 3000; acc: 99.3; ppl:   3.5; xent: 1.3; lr: 0.00173; sents:   49039; bsz: 3872/3622/123; 27082/25334 tok/s;   1760 sec;\n",
            "[2023-08-22 10:23:58,486 INFO] Step 2700/ 3000; acc: 99.3; ppl:   3.5; xent: 1.3; lr: 0.00170; sents:   49305; bsz: 3880/3630/123; 27026/25284 tok/s;   1817 sec;\n",
            "[2023-08-22 10:24:40,105 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 267\n",
            "[2023-08-22 10:24:40,422 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 268\n",
            "[2023-08-22 10:24:40,873 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 269\n",
            "[2023-08-22 10:24:41,406 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 270\n",
            "[2023-08-22 10:24:41,918 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 271\n",
            "[2023-08-22 10:24:45,465 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 272\n",
            "[2023-08-22 10:24:45,739 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 273\n",
            "[2023-08-22 10:24:46,014 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 274\n",
            "[2023-08-22 10:24:46,310 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 275\n",
            "[2023-08-22 10:24:46,582 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 276\n",
            "[2023-08-22 10:24:46,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 277\n",
            "[2023-08-22 10:24:47,122 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 278\n",
            "[2023-08-22 10:24:47,401 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 279\n",
            "[2023-08-22 10:24:47,667 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 280\n",
            "[2023-08-22 10:24:47,941 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 281\n",
            "[2023-08-22 10:24:48,213 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 282\n",
            "[2023-08-22 10:24:48,488 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 283\n",
            "[2023-08-22 10:24:48,757 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 284\n",
            "[2023-08-22 10:24:49,027 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 285\n",
            "[2023-08-22 10:24:52,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 286\n",
            "[2023-08-22 10:24:53,139 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 287\n",
            "[2023-08-22 10:24:53,432 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 288\n",
            "[2023-08-22 10:24:53,708 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 289\n",
            "[2023-08-22 10:24:53,992 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 290\n",
            "[2023-08-22 10:24:54,451 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 291\n",
            "[2023-08-22 10:24:54,978 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 292\n",
            "[2023-08-22 10:24:55,499 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 293\n",
            "[2023-08-22 10:24:56,013 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 294\n",
            "[2023-08-22 10:24:56,547 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 295\n",
            "[2023-08-22 10:24:57,072 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 296\n",
            "[2023-08-22 10:24:57,591 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 297\n",
            "[2023-08-22 10:24:58,075 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 298\n",
            "[2023-08-22 10:24:58,386 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 299\n",
            "[2023-08-22 10:24:58,667 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 300\n",
            "[2023-08-22 10:24:58,934 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 301\n",
            "[2023-08-22 10:25:03,294 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 302\n",
            "[2023-08-22 10:25:03,591 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 303\n",
            "[2023-08-22 10:25:03,865 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 304\n",
            "[2023-08-22 10:25:35,678 INFO] Step 2800/ 3000; acc: 99.3; ppl:   3.5; xent: 1.3; lr: 0.00167; sents:   48706; bsz: 3880/3632/122; 15969/14947 tok/s;   1914 sec;\n",
            "[2023-08-22 10:26:33,275 INFO] Step 2900/ 3000; acc: 99.4; ppl:   3.5; xent: 1.2; lr: 0.00164; sents:   49436; bsz: 3872/3624/124; 26893/25172 tok/s;   1972 sec;\n",
            "[2023-08-22 10:27:30,767 INFO] Step 3000/ 3000; acc: 99.1; ppl:   3.5; xent: 1.3; lr: 0.00161; sents:   48878; bsz: 3879/3628/122; 26989/25242 tok/s;   2029 sec;\n",
            "[2023-08-22 10:27:31,230 INFO] valid stats calculation and sentences rebuilding\n",
            "                           took: 0.4620704650878906 s.\n",
            "[2023-08-22 10:27:31,232 INFO] Train perplexity: 5.74347\n",
            "[2023-08-22 10:27:31,232 INFO] Train accuracy: 89.9924\n",
            "[2023-08-22 10:27:31,232 INFO] Sentences processed: 1.47688e+06\n",
            "[2023-08-22 10:27:31,232 INFO] Average bsz: 3877/3628/123\n",
            "[2023-08-22 10:27:31,232 INFO] Validation perplexity: 106.838\n",
            "[2023-08-22 10:27:31,232 INFO] Validation accuracy: 42.2054\n",
            "[2023-08-22 10:27:31,232 INFO] Stalled patience: 3/4\n",
            "[2023-08-22 10:27:31,235 INFO] Saving checkpoint models/model.fren_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For error debugging try:\n",
        "# !dmesg -T"
      ],
      "metadata": {
        "id": "XUYAvE8ffK2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eShpS01j-Jcp"
      },
      "source": [
        "# Translation\n",
        "\n",
        "Translation Options:\n",
        "* `-model` - specify the last model checkpoint name; try testing the quality of multiple checkpoints\n",
        "* `-src` - the subworded test dataset, source file\n",
        "* `-output` - give any file name to the new translation output file\n",
        "* `-gpu` - GPU ID, usually 0 if you have one GPU. Otherwise, it will translate on CPU, which would be slower.\n",
        "* `-min_length` - [optional] to avoid empty translations\n",
        "* `-verbose` - [optional] if you want to print translations\n",
        "\n",
        "Refer to [OpenNMT-py translation options](https://opennmt.net/OpenNMT-py/options/translate.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbQEGTj4TybH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d896c17e-0ebb-44f8-a826-513d1e842071"
      },
      "source": [
        "# Translate the \"subworded\" source file of the test dataset\n",
        "# Change the model name, if needed.\n",
        "!onmt_translate -model models/model.fren_step_3000.pt -src naga.txt-filtered.ng.subword.test -output en.translated -gpu 0 -min_length 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-08-22 12:26:13,995 INFO] Loading checkpoint from models/model.fren_step_3000.pt\n",
            "[2023-08-22 12:26:15,128 INFO] Loading data into the model\n",
            "[2023-08-22 12:26:25,637 INFO] PRED SCORE: -0.5759, PRED PPL: 1.78 NB SENTENCES: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHYihrgfIrIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10b33ac-326d-4a06-8190-2c29d1b9f7cd"
      },
      "source": [
        "# Check the first 5 lines of the translation file\n",
        "!head -n 5 en.translated"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñÅFor ‚ñÅthe ‚ñÅeager ‚ñÅexpect ation ‚ñÅof ‚ñÅthe ‚ñÅcreation ‚ñÅwait s ‚ñÅfor ‚ñÅthe ‚ñÅreveal ing ‚ñÅof ‚ñÅthe ‚ñÅsons ‚ñÅof ‚ñÅ unless ‚ñÅit .\n",
            "‚ñÅAs ‚ñÅhe ‚ñÅwas ‚ñÅwalking ‚ñÅby ‚ñÅthe ‚ñÅSea ‚ñÅof ‚ñÅGalilee , ‚ñÅhe ‚ñÅsaw ‚ñÅtwo ‚ñÅbrothers , ‚ñÅSimon ‚ñÅwho ‚ñÅis ‚ñÅcalled ‚ñÅPeter , ‚ñÅand ‚ñÅAndrew ‚ñÅhis ‚ñÅbrother , ‚ñÅcast ing ‚ñÅa ‚ñÅnet ‚ñÅinto ‚ñÅthe ‚ñÅsea , ‚ñÅfor ‚ñÅthey ‚ñÅwere ‚ñÅfishermen .\n",
            "‚ñÅLe t ‚ñÅthe ‚ñÅwho le ‚ñÅspirit ‚ñÅof ‚ñÅgentleness ‚ñÅbe ‚ñÅfulfilled ‚ñÅin ‚ñÅyour ‚ñÅheart , ‚ñÅand ‚ñÅlove ‚ñÅin ‚ñÅorder ‚ñÅto ‚ñÅbe ‚ñÅfulfilled ‚ñÅin ‚ñÅs uch ‚ñÅa ‚ñÅperson . ‚ñÅTh at ‚ñÅis ‚ñÅwhat ‚ñÅis ‚ñÅdetestable ‚ñÅthings ‚ñÅof ‚ñÅthe ‚ñÅoffering ‚ñÅof ‚ñÅGod .\"\n",
            "‚ñÅHi s ‚ñÅbrothers ‚ñÅtherefore ‚ñÅsaid , ‚ñÅ\" Where ‚ñÅcan ‚ñÅwe ‚ñÅget ‚ñÅenough ‚ñÅloaves ‚ñÅof ‚ñÅolive ‚ñÅoil ? ‚ñÅOr ‚ñÅout ? ‚ñÅOr ‚ñÅnaked , ‚ñÅor ‚ñÅstraw , ‚ñÅor ‚ñÅin ‚ñÅthe ‚ñÅsea , ‚ñÅor ‚ñÅin ‚ñÅwater .\n",
            "‚ñÅYou ‚ñÅhave ‚ñÅgiven ‚ñÅthem ‚ñÅmy ‚ñÅname , ‚ñÅso ‚ñÅthat ‚ñÅyou ‚ñÅare ‚ñÅ those ‚ñÅwho m ‚ñÅyou ‚ñÅhave ‚ñÅgiven ‚ñÅme . ‚ñÅYou ‚ñÅhave ‚ñÅgiven ‚ñÅme , ‚ñÅand ‚ñÅthey ‚ñÅhave ‚ñÅgiven ‚ñÅme ‚ñÅto ‚ñÅeveryone .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRsJm6UET2C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1c151d-1eba-413f-c481-67436ec65e86"
      },
      "source": [
        "# If needed install/update sentencepiece\n",
        "!pip3 install --upgrade -q sentencepiece\n",
        "\n",
        "# Desubword the translation file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model en.translated\n",
        "# !python3 MT-Preparation/subwording/3-desubword.py source.model naga.txt-filtered.ng.subword.test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: naga.txt-filtered.ng.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai4RhhGaKBp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9572d7e2-411a-4288-dc62-8be3a84d670b"
      },
      "source": [
        "# Check the first 5 lines of the desubworded translation file\n",
        "!tail -n 5 en.translated.desubword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jesus said to them, \"Do not know.\" Jesus said to them, \"Neither will I tell you by what authority I do these things. What will I do these things.\"\n",
            "In the same way, the women should be dignified, not double-talkers. They should not drink too much wine or be greedy.\n",
            "Let the one who has mercy and peace with you, have mercy on your way.\n",
            "That is whatever things they are with him, and all things are through him.\n",
            "He spoke many things to them about how he spoke a parable to them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUWB4r3OFOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11334561-7c47-4b2e-d5fa-0bc513423e07"
      },
      "source": [
        "# Desubword the target file (reference) of the test dataset\n",
        "# Note: You might as well have split files *before* subwording during dataset preperation,\n",
        "# but sometimes datasets have tokeniztion issues, so this way you are sure the file is really untokenized.\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model eng.txt-filtered.en.subword.test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: eng.txt-filtered.en.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jULN0MwOFeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c719c204-0acf-44f9-c871-31edb43c9aa2"
      },
      "source": [
        "# Check the first 5 lines of the desubworded reference\n",
        "!tail -n 5 eng.txt-filtered.en.subword.test.desubword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Then they answered Jesus and said, \"We do not know.\" Then Jesus said to them, \"Neither will I tell you by what authority I do these things.\"\n",
            "In the same way, their wives should be dignified, not slanderers, but sober and faithful in all things.\n",
            "To all who walk according to this standard, peace and mercy be upon them, even upon the Israel of God.\n",
            "He himself is before all things, and in him all things hold together.\n",
            "With many parables like this he spoke the word to them, as much as they were able to understand,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHMumxqvLDDc"
      },
      "source": [
        "# MT Evaluation\n",
        "\n",
        "There are several MT Evaluation metrics such as BLEU, TER, METEOR, COMET, BERTScore, among others.\n",
        "\n",
        "Here we are using BLEU. Files must be detokenized/desubworded beforehand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-9XGYnaJ-Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41203ad-612f-44f2-8585-a36f6de4ae8c"
      },
      "source": [
        "# Download the BLEU script\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-22 12:31:49--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‚Äòcompute-bleu.py‚Äô\n",
            "\n",
            "\rcompute-bleu.py       0%[                    ]       0  --.-KB/s               \rcompute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-22 12:31:49 (42.6 MB/s) - ‚Äòcompute-bleu.py‚Äô saved [957/957]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYDG0x0KLk_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc6e508-ca78-4594-a7d4-f4fe4ddb4674"
      },
      "source": [
        "# Install sacrebleu\n",
        "!pip3 install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3V3tZphTzK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0fe973-36a8-410c-ad9a-2fead0988d23"
      },
      "source": [
        "# Evaluate the translation (without subwording)\n",
        "!python3 compute-bleu.py eng.txt-filtered.en.subword.test.desubword en.translated.desubword"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: For we know that the whole creation groans and labors in pain together even now.\n",
            "MTed 1st sentence: For the eager expectation of the creation waits for the revealing of the sons of unless it.\n",
            "BLEU:  14.774036066234434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBi1PhRv4bX9"
      },
      "source": [
        "# More Features and Directions to Explore\n",
        "\n",
        "Experiment with the following ideas:\n",
        "* Icrease `train_steps` and see to what extent new checkpoints provide better translation, in terms of both BLEU and your human evaluation.\n",
        "\n",
        "* Check other MT Evaluation mentrics other than BLEU such as [TER](https://github.com/mjpost/sacrebleu#ter), [WER](https://blog.machinetranslation.io/compute-wer-score/), [METEOR](https://blog.machinetranslation.io/compute-bleu-score/#meteor), [COMET](https://github.com/Unbabel/COMET), and [BERTScore](https://github.com/Tiiiger/bert_score). What are the conceptual differences between them? Is there special cases for using a specific metric?\n",
        "\n",
        "* Continue training from the last model checkpoint using the `-train_from` option, only if the training stopped and you want to continue it. In this case, `train_steps` in the config file should be larger than the steps of the last checkpoint you train from.\n",
        "```\n",
        "!onmt_train -config config.yaml -train_from models/model.fren_step_3000.pt\n",
        "```\n",
        "\n",
        "* **Ensemble Decoding:** During translation, instead of adding one model/checkpoint to the `-model` argument, add multiple checkpoints. For example, try the two last checkpoints. Does it improve quality of translation? Does it affect translation seepd?\n",
        "\n",
        "* **Averaging Models:** Try to average multiple models into one model using the [average_models.py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/bin/average_models.py) script, and see how this affects translation quality.\n",
        "```\n",
        "python3 average_models.py -models model_step_xxx.pt model_step_yyy.pt -output model_avg.pt\n",
        "```\n",
        "* **Release the model:** Try this command and see how it reduce the model size.\n",
        "```\n",
        "onmt_release_model --model \"model.pt\" --output \"model_released.pt\n",
        "```\n",
        "* **Use CTranslate2:** For efficient translation, consider using [CTranslate2](https://github.com/OpenNMT/CTranslate2), a fast inference engine. Check out an [example](https://gist.github.com/ymoslem/60e1d1dc44fe006f67e130b6ad703c4b).\n",
        "\n",
        "* **Work on low-resource languages:** Find out more details about [how to train NMT models for low-resource languages](https://blog.machinetranslation.io/low-resource-nmt/).\n",
        "\n",
        "* **Train a multilingual model:** Find out helpful notes about [training multilingual models](https://blog.machinetranslation.io/multilingual-nmt).\n",
        "\n",
        "* **Publish a demo:** Show off your work through a [simple demo with CTranslate2 and Streamlit](https://blog.machinetranslation.io/nmt-web-interface/).\n"
      ]
    }
  ]
}